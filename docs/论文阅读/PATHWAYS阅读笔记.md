# PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML 

# 针对机器学习的异步分布式数据流阅读笔记

## 摘要

本篇论文介绍了用于加速器的新的大规模的编排层orchestration layer的设计——pathways。我们的系统（Pathways）是明确设计的，以实现对新系统和ML研究思想的探索，同时保留当前模型的最先进的性能。

新型异步分布式数据流

## 1. 导言

### 1.1 背景

近10年里的趋势是机器学习的算法和系统与硬件一起演进（co-evolution）这种co-evolution有一定的隐患：系统过于针对当前的任务，而不能很好地适应未来的需求。

pathways：针对的是分布式机器学习，具有的相关特性会被未来的任务所需要。

目前的分布式机器学习系统的现状：当今大多数最先进的ML工作负载都使用MPI的**“单程序多数据”（SPMD）**模型。所谓的SPMD可以类比2014年OSDI的另一篇论文参数服务器中的例子：多机在完成梯度的计算后交换信息，做一次数据的同步。

### 1.2 动机

未来ML需要一些特定的功能，哪些特性是未来ML所需要的？

- **模型较大**：对于大语言模型模型在一个加速器上是不够的，纯粹的数据并行是不够的

  开始考虑流水线（pipeline）而不是纯的数据并行（文中以MoE为对象介绍，他是一种MPI）

- **存在计算稀疏性**：使用细粒度的控制流和夸加速器的异构计算

- **硬件异构**：就硬件上的变化来说，机器学习集群变得越来越异构

针对硬件的异构性，论文中提出了一个“岛”(island)的概念，<u>**“岛”指的是一组连接在一起的加速器**</u>。单机的加速器之间连接较为紧密，而多机之间的加速器连接并不紧密。如果需要维持多机之间紧密的联系需要耗费较大的带宽，这通常是比较浪费的：用户需要试图保持所有加速器持续繁忙。

这种异构性的存在，进一步推动研究人员转向**“多程序多数据”(MPMD)**计算，通过**将整体计算的子部分映射到更容易获得的小型加速器岛的集合，从而实现更大的灵活性**。为了提高利用率，一些ML硬件资源管理人员以细粒度的方式在工作负载之间复用硬件，从而支持工作负载弹性，并提高容错能力。

最后，研究人员开始标准化一套基础模型，这些模型是在大数据上大规模训练的，可以适应多种下游任务。对此类模型的训练和推断提供了通过跨许多任务多路复用资源来提高集群利用率的机会，并有效地在它们之间共享状态。

<img src="https://cdn.jsdelivr.net/gh/Holmes233666/blogImage@main/img/2022/08/10/478db6744763d75e1cb68d00f4ca3075-image-20220810195437069-cf4f2a.png" alt="image-20220810195437069" style="zoom:50%;" />

结合Jeff Dean去年的博客[Pathways: A next-generation AI architecture (blog.google)](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)（上图），能够较好地理解这个**基础模型**的概念。

### 1.3 贡献

Pathways系统与最先进的ML系统状态的功能和性能匹配，支持未来ML工作负载所需要的功能。Pathways使用的体系结构为client-server体系结构。这个体系结构使得Pathways运行时能够代表许多客户在系统管理的计算岛上执行程序。

创新性：

- Pathways旨在透明有效执行跨越TPU的"POD"程序的系统
- 采用数据流执行（DataFlow Execution）模型来扩展到数千个加速器

Pathways使非SPMD计算变得容易，并启用了CENLALALIZED资源管理和虚拟化，改善了加速器的利用率。



## 2. 设计动机

### 2.1 现有ML系统的设计

现有的ML系统的设计难以支持大型的，稀疏的，或者不规则的模型。

#### 2.1.1 Multi-controller System

用于训练最先进的**SPMD模型**的分布式ML系统通常采用**多控制器（multi-controller）体系结构**，其中相同的客户机可执行文件直接在系统中的所有主机上运行，在程序执行期间独占这些主机上的资源
$$
训练最先进的SPMD的分布式ML系统例子\begin{cases}MPI\\Pytorth\\
JAX\\
TensorFlow\\
......\end{cases}
$$
这种架构的优点是计算的低延迟：因为用户代码的相同副本运行在每个加速器的主机上，并且分派补丁只涉及到通过相对快速的PCIe链路进行通信。所有其他跨主机的通信只通过使用专用互连的集合发生，如NVLink，而不使用主机内存。

缺点：

- 这种架构与**使用流水线**或者具有**计算稀疏性**的线代ML工作负载不匹配。
- multi-controller系统中，任何超过标准集合的通信需要用户实现他们自己的协调原语
- multi-controller通常假定独占硬件资源，使得构建高效的集群范围ML设计变得复杂

<img src="https://cdn.jsdelivr.net/gh/Holmes233666/blogImage@main/img/2022/08/10/c295e2c58b0a26a2747f9202245cd843-image-20220810225219876-1e898a.png" alt="image-20220810225219876" style="zoom: 50%;" />

#### 2.1.2 Single-controller System

单控制器（single-controller）系统，如TensorFlow v1提供了一个包括优化图内控制流的非常通用的分布式数据流模型。

TensorFlow (TF) Python客户端构建一个**计算图**，并将其移交给**协调运行时**，协调运行时将图划分为每个worker的子图，并将子图的执行委托给worker上的本地运行时。worker之间的协调是通过数据和控制边缘在数据中心网络(DCN)上传递消息来执行的。

虽然单控制器设计提供了灵活的编程模型和虚拟化资源，但它提出了**实现的挑战**：

##### 2.1.2.1 TF1 SPMD

- 通信更慢：单控制系统系统中的客户端更远，分派延迟包括DCN上的通信，通常比PCIe慢一个数量级
- controller为了更好的分派子图，可能会预先编译优化图，这样的优化会使**debug变得困难**
- 在涉及许多跨主机传输的程序中，例如使用大量的阶段，这些**调度延迟累积**，导致低效的加速器利用率。

<img src="https://cdn.jsdelivr.net/gh/Holmes233666/blogImage@main/img/2022/08/11/5fc7d4ce72ccb27c40ac820ae095f5c1-image-20220811111621366-f9ac99.png" alt="image-20220811111621366" style="zoom:50%;" />



##### 2.1.2.2 TF1 non-SPMD

- 为了支持带有SPMD的子计算的MPMD程序并发执行，运行时需要有某种机制来支持加速器计算的**分组调度**，否则可能会出现死锁。
- 计算结点变多，边变多，中央控制器指令收发成为一种瓶颈——带宽变低，通信量增多



<img src="https://cdn.jsdelivr.net/gh/Holmes233666/blogImage@main/img/2022/08/11/5164b5e21dab065833633c93189f3fc1-image-20220811111354323-8ceb0f.png" alt="image-20220811111354323" style="zoom:50%;" />

### 2.2 Pathways要实现的目标

而针对Pathways来说，Pathways并不是打算去解决TF1在分组调度、debug困难这样的**易用性**的问题，而是认为TF1这样的设计是合理的，即在系统上，把这样有一个中央controller，能够把任务分配到各个结点上。同时这个概念称为**dataflow**。——论文题目dataflow的由来

>  dataflow：
>
> 计算表示成一个计算的图，一个结点是一个计算，箭头表示依赖的关系。
>
> 系统的目标：给一个计算图，有向无环图，如何把他映射到硬件上，更好地调度执行。

**TF1受限的问题：TF1考虑的是比较小的图，是在几百个同构的加速器上执行。**

TF1 执行大图的问题

TF1中图的结点和边变多，结点进行的运算较少，中央控制器进行指令收发成为了一个瓶颈——带宽变低，通信量增多了 not good

现在模型更大，异构，如何修改设计？

- 仍采用单控制器（single controller）：利用计算稀疏性和异构性，促进资源共享和虚拟化
- **异步调度**来匹配**多控制器系统的性能**
- 支持集中资源管理和调度，使分片数据流系统高效协调

## 3. PATHWAYS 编程模型

>  Google深度学习框架前置知识：
>
> 第一代深度学习框架disBlief
>
> 第二代深度学习框架TensorFlow v1：调试不方便，性能优化方便；运行得到的是一张图，而不是中间的结果；运行完后得到的完整的执行图
>
> - 好处：后端可以进行性能优化；计算图与Python无关，灵活性提高
>
> - 缺点：调试变得困难
>
> TensorFlow v2：与TensorFlow v1不同，几乎是另一套框架
>
> 谷歌之后开始研究TPU，为TPU开发了**编译器XLA**；XLA之后也支持了GPU和TPU，是一个统一的后端
>
> JAX：XLA的前端，设计理念是提供一个与numpy类似的前端——可以按行执行，实际上采用的是一种延后执行的思想；
>
> - 从用户看：和Python按行执行似乎没有什么区别
> - 从后端：拿到很多图，只是图大小变小很多

### 3.1 JAX和TensorFlow对PATHWAYS支持

我们已经实现了用TensorFlow和JAX编写的源程序对PATHWAYS的支持，但在本文中我们将重点放在JAX上进行评估。JAX用户可以显式地使用decorator包装标准Python代码，以指示应该编译成(可能是SPMD) XLA计算的片段。这些XLA计算的特点通常是已知的输入和输出类型和形状，有界循环，并且很少(如果有的话)条件(详见附录B)，这使得**提前估计计算的资源需求是可行的**。我们将这些已知资源需求的计算称为**“编译函数”**。每个这样的函数都映射到PATHWAYS程序中的单个(分片)计算节点。

由于JAX程序在multi-controller配置下运行，使用XLA集合传输所有数据，因此目前JAX无法扩展到单个TPU之外，而XLA集合目前只能在TPU上的ICI上使用。**PATHWAYS可以用作JAX后端的插件替代品**，允许JAX代码不加修改地运行，只是SPMD计算现在不仅可以访问本地连接的TPU核心，而且可以访问系统中提供的所有核心。由于pathway可以通过ICI和DCN进行通信，这使得**JAX程序可以第一次扩展到多个TPU pods，其中包含了数千个TPU核心。**



首先声明：在什么上运行

pmap 并行的映射，函数映射到两个tpu上  get device返回两个虚拟的tpu

f函数触发了4次编译，触发了4次调度

tracing函数？

## Pathway 系统架构

### 4.1 资源管理器

也可以做成立方体的形式

![image-20220809204840041](https://cdn.jsdelivr.net/gh/Holmes233666/blogImage@main/img/image-20220809204840041.png)

一个pod里面tpu连接是非常快的，pod之间带宽较低，网络中心。

pathways：想用到多个pod，支持多机多卡的训练

怎么样把任务放到三个岛上运行

资源管理器：把虚拟的tpu映射到物理的tpu

未来支持动态的加上或者减去一些结点

### 4.2 Client   通讯

Pathways怎么把用户的任务编译，运行。

jax----->milr（XLA上层语言）----->汇编

在往下走时需要映射，做优化

rdma 内存被源端的机器使用

映射到底层的架构，rdma实现   rdma：直接访问机器的内存地址

内存管理：一块内存可能被远端机器使用

使用 shared buffer

### 4.3 Coordination implementation  管理内存

怎么样调度信息的传送

使用PLAQUE优化，也可以用Ray

### 4.4 Gang Schedule  减少任务派发带来的影响

tpu单线程

gang schedule对每一个岛有一个调度，使其不会发送死锁

现在使用FIFO实现

### 4.5 并行异步分发



### 4.6 数据管理

什么时候内存可以回收呢？

## 评估



